{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Random forrest for email spam classiﬁer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task for this question is to build a spam classiﬁer using the UCR email spma dataset https://archive. ics.uci.edu/ml/datasets/Spambase came from the postmaster and individuals who had ﬁled spam. The collection of non-spam e-mails came from ﬁled work and personal e-mails, and hence the word ’george’ and the area code ’650’ are indicators of non-spam. These are useful when constructing a personalized spam ﬁlter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam ﬁlter. Load the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.externals.six'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ca56a3cf9cf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.externals.six'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO \n",
    "from IPython.display import Image \n",
    "from pydot import graph_from_dot_data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, roc_curve\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split ,GridSearchCV,KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 (5 points) How many instances of spam versus regular emails are there in the data? How many data points there are? How many features there are? Note: there may be some missing values, you can just ﬁll in zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spambase = pd.read_csv('spambase.data',header=None)\n",
    "spambase = spambase.fillna(0)\n",
    "observations, variables = spambase.shape\n",
    "\n",
    "total_features = variables-1 \n",
    "\n",
    "spams = spambase[spambase.iloc[:,-1] == 1]\n",
    "nonspams = spambase[spambase.iloc[:,-1] == 0]\n",
    "\n",
    "print(\"Total Number of data points:{} \".format(observations))\n",
    "print(\"Total Variables:{} \".format(total_features))\n",
    "print(\"Total spam records: {}\".format(len(spams)))\n",
    "print(\"Total non-spam records: {}\".format(len(nonspams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 (10 points) Build a classiﬁcation tree model (also known as the CART model). In Python, this can be done using sklearn.tree.DecisionTreeClassiﬁer. In our answer, you should report the tree models ﬁtted similar to what is shown in the “Random forest” lecture, Page 16, the tree plot. In Python, getting this plot can be done using sklearn.tree.plot tree function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 50\n",
    "(Xtrain, Xtest, ytrain, ytest) = train_test_split(spambase.iloc[:, 0: -1], spambase.iloc[:, -1], test_size=0.2, random_state = random_seed )\n",
    "classification = tree.DecisionTreeClassifier(max_features='sqrt', random_state=random_seed, max_depth = 4)\n",
    "classification = classification.fit(Xtrain, ytrain)\n",
    "\n",
    "# Lets print the tree plot using plot_tree function\n",
    "\n",
    "tree.plot_tree(classification, filled = True )\n",
    "plt.figure(num=None, figsize=(1000, 200), dpi=80, facecolor='w', edgecolor='k')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. (15 points) Also build a random forrest model. In Python, this can be done using sklearn.ensemble.RandomForestClassiﬁer. Now partition the data to use the ﬁrst 80% for training and the remaining 20% for testing. Your task is to compare and report the AUC for your classiﬁcation tree and random forest models on testing data, respectively. To report your results, please try diﬀerent tree sizes. Plot the curve of AUC versus Tree Size, similar to Page 15 of the Lecture Slides on “Random Forest”. Background information: In classiﬁcation problem, we use AUC (Area Under The Curve) as a performance measure. It is one of the most important evaluation metrics for checking any classiﬁcation model?s performance. ROC (Receiver Operating Characteristics) curve measures classiﬁcation accuracy at various thresholds settings. AUC measures the total area under the ROC curve. Higher the AUC, better the model is at distinguishing the two classes. If you want to read a bit more about AUC curve, check out this link https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 For instance, in Python, this can be done using sklearn.metrics.roc auc score and you will have to ﬁgure out the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest = RandomForestClassifier(n_estimators=10, random_state=random_seed, max_features='sqrt')\n",
    "Random_Forest = Random_Forest.fit(Xtrain, ytrain)\n",
    "\n",
    "# Calculating AUC for Random Forest Classifer\n",
    "y_test_pred = Random_Forest.predict(Xtest)\n",
    "y_test_prob = Random_Forest.predict_proba(Xtest)[:,1]\n",
    "\n",
    "#Calculating AUC for Classification Tree Model\n",
    "clf_y_test_pred = classification.predict(Xtest)\n",
    "clf_y_test_prob = classification.predict_proba(Xtest)[:,1]\n",
    "\n",
    "print(f'Decision Tree : Test ROC AUC  Score: {roc_auc_score(ytest, clf_y_test_prob)}')\n",
    "print(f'Random Forest : Test ROC AUC  Score: {roc_auc_score(ytest, y_test_prob)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you compare the AUC of the Decision Tree Classifier with the Random Forest Classifier, it is clear that Random Forest performs better. \n",
    "However, we have provided \"UNLIMITED Depth\" to both classifiers which can easily result in extreme overfitting.\n",
    "Let's retrieve AUC score for both the method for varying number of \"Depth\" of Trees - ranging from 4 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc_by_max_depth(Xtrain, Xtest, ytrain, ytest, max_depth, random_seed):\n",
    "    # determine ROC for Decision Tree Classifer\n",
    "    classification = tree.DecisionTreeClassifier(max_features='sqrt', max_depth=max_depth, random_state=random_seed)\n",
    "    classification = classification.fit(Xtrain, ytrain)\n",
    "    classification_y_test_prob = classification.predict_proba(Xtest)[:,1]\n",
    "    classification_auc = roc_auc_score(ytest, classification_y_test_prob)\n",
    "    \n",
    "    # determine ROC for Random Forest Classifier\n",
    "    Random_Forest = RandomForestClassifier(n_estimators=10,max_depth=max_depth,max_features='sqrt',random_state=random_seed)\n",
    "    Random_Forest = Random_Forest.fit(Xtrain, ytrain)\n",
    "    Random_Forest_y_test_prob = Random_Forest.predict_proba(Xtest)[:,1]\n",
    "    Random_Forest_auc = roc_auc_score(ytest, Random_Forest_y_test_prob)\n",
    "    \n",
    "    return classification_auc, Random_Forest_auc\n",
    "\n",
    "# Range for depth of trees from 2 to 20, calculate ROC using both methods\n",
    "rec_list = []\n",
    "for n in range(2,22,2):\n",
    "    classification_auc,Random_Forest_auc = calc_roc_by_max_depth(Xtrain, Xtest, ytrain, ytest, n , random_seed)\n",
    "    #print(\"For depth = {} , clf_roc = {} rnf_roc = {} \".format(n,clf_roc,rnf_roc))\n",
    "    rec_list.append([n,classification_auc, 'Decision Tree Classifier'])\n",
    "    rec_list.append([n,Random_Forest_auc,'Random Forest Classifer'])\n",
    "    \n",
    "data_df = pd.DataFrame(rec_list, columns=['num_trees','auc','method'])\n",
    "xlabels=[0,2,4,6,8,10,12,14,16,18,20]\n",
    "ax = sns.lineplot(x = 'num_trees', y = 'auc', hue= 'method',data=data_df)\n",
    "ax.set(xlim=(0, 22))\n",
    "ax.set(title=\"AUC by depth of Tree-Levels\")\n",
    "ax.set_xticklabels(xlabels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Nonlinear regression and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of thermal expansion y changes with temperature x. An experiment to relate y to x was done. Temperature was measured in degrees Kelvin. (The Kelvin temperature is the Celcius temperature plus 273.15). The raw data file is copper-new.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. (10 points) Perform linear regression on the data. Report the ﬁtted model and the ﬁtting error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copper_data=pd.read_csv('copper-new.txt',header=None,delimiter = ' ',skipinitialspace = True,engine='python', \n",
    "                        names=['coefficient','temperature'])\n",
    "linear_regressor = LinearRegression()\n",
    "x = np.reshape(np.asarray(copper_data['temperature']) ,(-1,1))\n",
    "y = np.reshape(np.asarray(copper_data['coefficient']) ,(-1,1))\n",
    "linear_regressor.fit(x,y) \n",
    "\n",
    "ypredict = linear_regressor.predict(x)\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y , ypredict))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y , ypredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2  Perform nonlinear regression with polynomial regression function up to degree n = 10 and use ridge regression (see Lecture Slides for \"Bias-Variance Tradeoff\"). Write down your formulation and strategy for doing this, the form of the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['yellow','greenyellow','green','aqua', 'lightblue', 'deepskyblue','violet','darkkhaki','lightcoral','red']\n",
    "polynomial_features= PolynomialFeatures(degree=10)\n",
    "model = make_pipeline(polynomial_features, Ridge())\n",
    "model.fit(np.reshape(np.asarray(copper_data['coefficient']) ,(-1,1)) ,copper_data['coefficient'] )\n",
    "for deg in range(10) :\n",
    "    degree= deg+1\n",
    "    polynomial_features= PolynomialFeatures(degree=degree)\n",
    "    model = make_pipeline(polynomial_features, Ridge(normalize = True))\n",
    "    model.fit(np.reshape(np.asarray(copper_data['temperature']) ,(-1,1)) ,copper_data['coefficient'] )\n",
    "    yplot = model.predict(np.reshape(np.asarray(copper_data['temperature']) ,(-1,1)) )\n",
    "    plt.plot(np.reshape(np.asarray(copper_data['temperature']) ,(-1,1)) , yplot, color=colors[deg],\n",
    "            label=\"degree %d\" % degree)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. (10 points) Use 5 fold cross validation to select the optimal regularization parameter Lambda .Plot the cross validation curve and report the optimal lambda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using 5-fold cross validation and degree of 10\n",
    "X = np.reshape(np.asarray(copper_data['temperature']) ,(-1,1))\n",
    "y = np.reshape(np.asarray(copper_data['coefficient']) ,(-1,1))\n",
    "alpha_values=[ 0.001, 0.01, 0.1 , 0.5 ,1, 5 ]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "polynomial_features= PolynomialFeatures(degree=10)\n",
    "ridge_mse=[]\n",
    "for a in alpha_values:\n",
    "    ridge_cv_model = Ridge(alpha=a, normalize=True)  \n",
    "    cv_mse=[]\n",
    "    for train_idx,test_idx in kf.split(X):\n",
    "        x_train_cv ,x_test_cv = X[train_idx], X[test_idx] \n",
    "        y_train_cv ,y_test_cv = y[train_idx], y[test_idx] \n",
    "        x_train_poly=polynomial_features.fit_transform(x_train_cv)\n",
    "        x_test_poly=polynomial_features.fit_transform(x_test_cv)\n",
    "        ridge_cv_model.fit(x_train_poly, y_train_cv)\n",
    "        y_pred_cv=ridge_cv_model.predict(x_test_poly)\n",
    "        theta=ridge_cv_model.coef_\n",
    "        mse=np.sum((y_pred_cv - y_test_cv)**2)/len(y_pred_cv) + a*np.sum(theta**2)\n",
    "        cv_mse.append(mse)\n",
    "    \n",
    "    ridge_mse.append((a, np.mean(cv_mse)))\n",
    "df = pd.DataFrame(ridge_mse, columns =['alpha_values','mse']) \n",
    "df2= df[df.mse == df.mse.min()]\n",
    "Optimal_Lambda = df2.iloc[0]['alpha_values']\n",
    "print(\"Optimal value of Lambda is:\",Optimal_Lambda )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels=alpha_values\n",
    "ax = sns.lineplot(x = 'alpha_values', y = 'mse',data=df)\n",
    "ax.set(title=\"alpha vs mse\")\n",
    "ax.set_xticklabels(xlabels);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. (10 points) Predict the coefcient at 400 degree Kelvin using both models. Comment on how would you compare the accuracy of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor_pred = linear_regressor.predict(np.reshape([400],(-1,1)))\n",
    "print('Linear Regression Coefficient: ',linear_regressor_pred[0][0])\n",
    "non_linear_regressor= model.predict(np.reshape([400],(-1,1)))\n",
    "print('Non linear regressor with polynomial regression Coefficient: ',non_linear_regressor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_pred = linear_regressor.predict(x)\n",
    "print(\"Linear Regressor Accuracy: \",round(r2_score(y, linear_reg_pred,multioutput='variance_weighted')*100,2),'%')\n",
    "non_linear_reg_pred = model.predict(x)\n",
    "print(\"Non-Linear Regressor Accuracy:\",round(r2_score(y, non_linear_reg_pred,multioutput='variance_weighted')*100,2),'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
